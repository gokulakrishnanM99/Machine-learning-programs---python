{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "cae69541658b406d495bcffe0b7c0932",
          "grade": false,
          "grade_id": "cell-aa1aa8c8181f8810",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "yBCjqrUIrZyX"
      },
      "source": [
        "# ***Basic Functions used in Machine learning algorithms***\n",
        "\n",
        "# Normalization, Prediction, Loss function with and without regularization, Gradient of those loss function, Root mean square metrics, Gradient decent and Pseudo inverse method this are the functions contained in this module.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "0ecbf8e96219ab11b7716eaae61cb7f9",
          "grade": false,
          "grade_id": "cell-1e940101f37c7af2",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "HQJm6OLcrZyY"
      },
      "source": [
        "## Import Statements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "fe70877a859ee0a10e9d591778022f8a",
          "grade": false,
          "grade_id": "Import_Statements",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "UIthSh0YrZyY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "e36a214573fc40f26b45e72215d61375",
          "grade": false,
          "grade_id": "Normalize_Title",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "04WMvmaVrZyZ"
      },
      "source": [
        "## Normalize function \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "42293dda356d746ac19e9b6d81106261",
          "grade": false,
          "grade_id": "Normalize_Function",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "NtFM4difrZyZ"
      },
      "outputs": [],
      "source": [
        "def Normalize(X): # Output will be a normalized data matrix of the same dimension\n",
        "    '''\n",
        "    Normalize all columns of X using mean and standard deviation\n",
        "    '''\n",
        "    y = (X - X.mean(axis=0)) / X.std(axis=0)\n",
        "    # Taking mean by averaging all elements in single column(i.e, generating an additional row having mean for each column) and subract that mean from the given data X\n",
        "    # Resultant value is divided by the Standard deviation(Std also find column elements, same as in mean)\n",
        "    return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "63ee774a26a0438a0b2f5ec298eac80e",
          "grade": false,
          "grade_id": "cell-66ab98f84d3c58fa",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "P-wjTtnkrZya"
      },
      "source": [
        "## Prediction Function\n",
        "\n",
        "Given X and w, compute the predicted output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5976e32bd96bc4bdc2b2efd61e98441c",
          "grade": false,
          "grade_id": "Prediction",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "QsUmsudrrZya"
      },
      "outputs": [],
      "source": [
        "def Prediction (X, w): # Output will be a prediction vector y\n",
        "    '''\n",
        "    Computing Prediction given an input data matrix X and weight vecor w. Output y = [X 1]w where 1 is a vector of all 1s \n",
        "    '''\n",
        "    X = np.append(X, np.ones((len(X), 1), dtype = int), axis = 1)\n",
        "    # appending a column of integer 1 to each row of x. Adding bias value [x 1] to X used to solve for w0 in w vector\n",
        "    predict = (w).dot(X.T)\n",
        "    #Matrix dot product between weight vector w and X\n",
        "    return predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "227f6e0da2f8c6d631599c44041e3b65",
          "grade": false,
          "grade_id": "cell-6fda2832d5967072",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "aHI7Kg9ZrZyb"
      },
      "source": [
        "## Loss Functions\n",
        "\n",
        "Code for the four  loss functions:\n",
        "\n",
        "1. MSE loss is only for the error\n",
        "2. MAE loss is only for the error\n",
        "3. L2 loss is for MSE and L2 regularization, and can call MSE loss\n",
        "4. L1 loss is for MSE and L1 regularization, and can call MSE loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2c3e17b8559f0dfdfecdc8d242aba152",
          "grade": false,
          "grade_id": "MSE_Loss",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "SeCSI81yrZyb"
      },
      "outputs": [],
      "source": [
        "def MSE_Loss (X, t, w, lamda =0):\n",
        "    '''\n",
        "    lamda=0 is a default argument to prevent errors if you pass lamda to a function that doesn't need it by mistake. \n",
        "    This allows us to call all loss functions with the same input format.\n",
        "    \n",
        "    You are encouraged read about default arguments by yourself online if you're not familiar.\n",
        "    '''\n",
        "    mse = (np.sum(np.square(Prediction(X,w) - t))) / len(X)\n",
        "    #As per the formula for mea square error: \n",
        "    #prediction(X, w) gives predicted y and target t is subracted from y\n",
        "    #For each value of y & t the difference is squared and added together\n",
        "    #the result is divided with n => len(x)\n",
        "    return mse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b0b3b381c7a3ac5d7dfbb5df647c0d85",
          "grade": false,
          "grade_id": "MAE_Loss",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "YwBERWWlrZyc"
      },
      "outputs": [],
      "source": [
        "def MAE_Loss (X, t, w, lamda = 0): \n",
        "    mae = (np.sum(np.abs(Prediction(X, w) - t))) / len(X)\n",
        "    #As similar in MSE, instead of Squaring, here absolute value (y - t) is taken, added together and divided by n\n",
        "    return mae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a8309762ea56bb6b24bbbf4a19cb16bc",
          "grade": false,
          "grade_id": "L2_Loss",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "vUvnLXpprZyc"
      },
      "outputs": [],
      "source": [
        "def L2_Loss (X, t, w, lamda): # Output should be a single number based on L2-norm (with sqrt)\n",
        "    l2 = MSE_Loss(X, t, w, lamda) + (lamda) * (np.sqrt(np.sum(np.square(np.abs(w[:len(w) - 1])))))\n",
        "    #l2 = MSE_Loss(X, t, w, lamda) + (lamda) *  (np.linalg.norm(w[: len(w) - 1]))\n",
        "    return l2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "1af69c123815524cdc71c72e09ece638",
          "grade": false,
          "grade_id": "L1_Loss",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "3fbYrTiirZyc"
      },
      "outputs": [],
      "source": [
        "def L1_Loss (X, t, w, lamda):\n",
        "    l1 = MSE_Loss(X, t, w, lamda) + (lamda) * (np.sum(np.abs(w[:len(w) - 1])))\n",
        "    return l1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "855bd2ded492e567b20f1d9705d9a97a",
          "grade": false,
          "grade_id": "NRMSE_Metric",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "bn1h8gHvrZyd"
      },
      "outputs": [],
      "source": [
        "def NRMSE_Metric (X, t, w, lamda=0): #RMSE/std_dev(t)\n",
        "    nrmse = np.sqrt(MSE_Loss(X, t, w)) / np.std(t)\n",
        "    return nrmse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "cc5df30e29bbe17d4bc7aa89210eb5cf",
          "grade": false,
          "grade_id": "Gradient_Title",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "BilrffcyrZyd"
      },
      "source": [
        "## Gradient function\n",
        "Each Loss function will have its own gradient function:\n",
        "\n",
        "1. MSE gradient is only for the error\n",
        "2. MAE gradient is only for the error\n",
        "3. L2 gradient is for MSE and L2 regularization, and can call MSE gradient\n",
        "4. L1 gradient is for MSE and L1 regularization, and can call MSE gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "0190492ee216edce701f93e697572fc1",
          "grade": false,
          "grade_id": "MSE_Gradient",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "gj6zIOG0rZyd"
      },
      "outputs": [],
      "source": [
        "def MSE_Gradient (X, t, w, lamda=0): # Output will have the same size as w\n",
        "    x = np.append(X, np.ones((len(X), 1), dtype = int), axis = 1)\n",
        "    mseg = (x.T).dot((Prediction(X, w) - t)) * 2 / len(t)\n",
        "    return mseg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "49cc4e37cafad52f41853b4b0e71f89f",
          "grade": false,
          "grade_id": "MAE_Gradient",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "QQ2GeLZ1rZye"
      },
      "outputs": [],
      "source": [
        "def MAE_Gradient (X, t, w, lamda=0): # Output will have the same size as w\n",
        "    x = np.append(X, np.ones((len(X), 1), dtype = int), axis =1)\n",
        "    maeg = (x.T).dot(np.sign(Prediction(X, w) - t)) / len (X)\n",
        "    return maeg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "595829c0dd17df55a03d346fac507b4a",
          "grade": false,
          "grade_id": "L2_Gradient",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "9Sl945F-rZye"
      },
      "outputs": [],
      "source": [
        "def L2_Gradient (X, t, w, lamda): # Output will have the same size as w\n",
        "    l2 = MSE_Gradient(X, t, w, lamda) \n",
        "    w[len(w) - 1] = 0\n",
        "    g = (lamda / np.sqrt(np.sum(np.square(w[:len(w) - 1])))) * (w)\n",
        "    l2g = l2 + g\n",
        "    return l2g"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "141736f1feda5aa225e008d749f0015c",
          "grade": false,
          "grade_id": "L1_Gradient",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "Jr4NyO7TrZye"
      },
      "outputs": [],
      "source": [
        "def L1_Gradient (X, t, w, lamda): # Output will have the same size as w\n",
        "    l1 = MSE_Gradient(X, t, w, lamda) \n",
        "    w[len(w) - 1] = 0\n",
        "    g = lamda * np.sign(w)\n",
        "    l1g = l1 + g\n",
        "    return l1g"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "1980a2f831b5e1ddc9b510c22ef502c7",
          "grade": false,
          "grade_id": "Gradient_Desc_Title",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "F87Bsw13rZyf"
      },
      "source": [
        "## Gradient Descent Function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "78cdd5ffa4590c10e19281722ccd537f",
          "grade": false,
          "grade_id": "Gradient_Descent",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "FGgtdznXrZyf"
      },
      "outputs": [],
      "source": [
        "def Gradient_Descent (X, X_val, t, t_val, w, lamda, max_iter, epsilon, lr, lossfunc, gradfunc): # See output format in 'return' statement\n",
        "    train_loss = lossfunc(X, t, w, lamda)\n",
        "    for i in range(max_iter):\n",
        "      w_final = w - (lr) * gradfunc(X, t, w, lamda)\n",
        "      w = w_final\n",
        "      train_loss_final = lossfunc(X, t, w, lamda)\n",
        "      validation_loss_final = lossfunc(X_val, t_val, w_final, lamda)\n",
        "      validation_NRMSE = NRMSE_Metric(X_val, t_val, w_final)\n",
        "      if validation_loss_final <= epsilon:\n",
        "        break\n",
        "    return w_final, train_loss_final, validation_loss_final, validation_NRMSE #It will return variables structured like this\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "24f327aa6a14590077f907ee8323724c",
          "grade": false,
          "grade_id": "PseudoInvTitle",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "ww_swOAHrZyg"
      },
      "source": [
        "## Pseudo Inverse Method\n",
        "\n",
        "It is a slightly more advanced version, with L2 penalty:\n",
        "\n",
        "w = (X' X + lambda I)^(-1) X' t.\n",
        "\n",
        "See, for example: Section 2 of https://web.mit.edu/zoya/www/linearRegression.pdf\n",
        "\n",
        "Here, the column of 1's in assumed to be included in X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "928c9b105c4bcf3c132c7520a140d509",
          "grade": false,
          "grade_id": "PseudoInv",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "-D_e_psjrZyg"
      },
      "outputs": [],
      "source": [
        "def Pseudo_Inverse (X, t, lamda): # Output will be weight vector\n",
        "    x = np.append(X, np.ones((len(X), 1), dtype = int), axis =1)\n",
        "    xt = (x.T).dot(x)\n",
        "    Inv = ((np.linalg.inv((xt + ((lamda) * (np.identity(len(xt))))) * (1 / len(t)))) * (1 / len(t))).dot(x.T).dot(t)\n",
        "    return Inv"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Basic Functions for Machine learning algorithms.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}